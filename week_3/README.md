## **决策树**
- 一棵决策树包含一个根节点，若干个内部节点和若干个叶节点。叶节点对于于决策结果，而其他每个节点则对应于一个属性测试。
 每个节点包含的样本集合根据属性测试分到子节点中，根节点包含样本节点全集。
 
------------
- 涉及问题
1. 属性测试划分，应该如何选择划分属性
1. 如何处理拟合问题。
1. 属性集中使用连续属性和当属性值缺失时如何处理。

------------

- 决策树的基本流程遵循“分而治之”的策略。
`

    	输入: 训练集D={(x_1，y_1),(x_2,y_2),...,(x_m,y_m)}
    	属性集A={a_1,a_2,...a_d}
    	过程：函数TreeGenerate(D,A)
   	 生成节点node;
   	 if D中的样本属于同一类别C：
   			将node标记为C类叶节点；return；
   	 if A==空集 or D中样本在A中的取值都相同
   			将node标记为叶节点，其类别为D中样本数最多的类；return；
    	从A中选择最优划分属性a*
   	for a* 的每个值 a*v:
  	 		为node生成一个分支，令D_v表示D中在a上取值为a*v的样本子集；
    			if D_v 为空
    					将分支标记为叶节点，其类别为D中样本数最多的类；return；
    			else：
    					以TreeGenerate(D_v, A\{a*});
    	输出：以node为根节点的一棵决策树。`
- 决策树基本算法是一个递归的过程，三种情况导致递归返回。
1. 当前节点包含的样本树全属于同一类别。
1. 当前属性集为空或者当前所有样本在所有属性上取值相同，无法划分。
（分类为含样本数最多的类）
1. 当前节点包含的样本集合为空。
（把当前节点标记为叶节点，但类别设定为其父节点所含样本最多的类别）

- 属性划分选择
离散属性a有V个取值{a_1,...a_v}。
若使用a属性来划分样本集D，则有V个分支节点，其中第v个节点包含了D中所有在属性a上取值为a_v的样本，记为D_v。
考虑到不同分支所包含的样本数不同，给分支节点赋予权重|D_v| / |D|，即样本数越多的分支节点影响越大。
1. 信息增益（ID3算法）
	D: 当前样本集
	k：样本第k类。k取值为1~|y|
	P：D中k类样本所占比例
	V: 属性a的可能取值
	信息熵：
	
	$$Ent(D)=-\sum_{k=1}^{|y|}{P_klog_2P_k}$$
	
	信息增益：
	
	$$Gain(D,a)=Ent(D)-\sum_{v=1}^V{\frac{|D^v|}{|D|}Ent(D^v)}$$
	- 信息增益越大，使用该属性划分所获得的纯度提升越大。
	信息增益准则对可取值数目较多的属性有所偏好
1. 








