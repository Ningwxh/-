# 支持向量机
1. 间隔与支持向量


- 对于分类学习得想法就是基于训练集在样本空间中找到一个划分超平面，将不同类别得样本分开。但超平面有很多，如何找个一个合适的超平面呢？
	![](pic/超平面.png)
	如图示，红色的线是我们要找的理想的超平面，因为该划分超平面对训练样本局部扰动的**容忍性**最好, 在样本空间中，划分超平面可由如下线性方程描述：

$$w^Tx+b=0$$

- 其中w和b决定了划分超平面。样本空间中任意点到超平面的距离有：

$$r=\frac{|w^Tx+b|}{||w||}$$

其中$$||w||=\sqrt{\sum_{j=1}{w_j^2}}$$

若超平面能将训练样本正确划分，即对 $$(x_i,y_i)$$属于 $$D$$ ，有：

$$w^T+b>=+1, y_i=+1$$

$$w^T+b<=-1,y_i=-1$$

- 距离超平面最近的几个训练样本点使式的等号成立，它们被称为“支持向量”，两个异类支持向量到超平面的和称为间隔：

$$\frac{2}{||w||} = \gamma$$

欲找到具有“最大间隔”的划分超平面，也就是找满足上诉不等式中的约束参数w和b。（通过二次规划，拉格朗日乘子法得其对偶问题求解，看不懂，😭）

*2.软间隔与正则化*
- 前面的叙述我们一直假定样本空间式线性可分得，即存在一个超平面能将不同的类**完全**划分，然后现实很难找到这样的划分超平面，所有引入软间隔概念。（数学理解不在此讨论，等看懂了再说。😭）
总而概知，为解决非线性问题，我们**放松线性约束条件**，以保证在适当的惩项成本下，对错误分类的情况进行优化时能够收敛。引入松弛变量ξ

$$w^Tx^{(i)}>=1, y^{(i)}=1-\xi^{(i)}$$

$$w^Tx^{(i)}<=1, y^{(i)}=1+\xi^{(i)}$$

每个样本对于一个松弛变量ξ，在新的约束条件下，新的优化目标为最小值化下式：

$$\frac{1}{2}||w||^2+C(\sum_{i}{\xi^{(i)}})$$

通过**变量C**，我们可以控制对错误分类的惩罚程度，**C值较大**时，对应**大的错误惩罚**，**C值较小**时，则对错误分类的**惩罚较低**。

*3.核函数*
- 现实世界中，原始样本空间可能不存在一个能够正确划分样本的超平面，如“异或”问题。
对此我们将原始空间映射到一个更高维度的特征空间中，使得样本在这个特征空间内线性可分。通过映射函数将样本映射到高维的特征空间，并在新的特征空间中训练SVM。这样的一个**映射函数叫做核函数**。有多种核函数可选择。




